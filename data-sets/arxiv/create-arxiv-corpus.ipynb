{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7776cf85",
   "metadata": {},
   "source": [
    "# Custom arXiv corpus for Annif\n",
    "## Subject vocabulary\n",
    "\n",
    "To create the subject vocabulary we first need the categories that are used in the arXiv articles. The categories can be browsed at [Category Taxonomy page](https://arxiv.org/category_taxonomy). However, they defined in a Python source file in a [arXiv's GitHub repository](https://github.com/arXiv/arxiv-base/blob/develop/arxiv/taxonomy/definitions.py), which can be easily downloaded. \n",
    "\n",
    "For downloading we use command-line tool `curl`, and to use the (terminal) command line from within Jupyter Notebook cell the command is prepended with `!`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00015482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89696  100 89696    0     0   222k      0 --:--:-- --:--:-- --:--:--  222k\n"
     ]
    }
   ],
   "source": [
    "! curl https://raw.githubusercontent.com/arXiv/arxiv-base/develop/arxiv/taxonomy/definitions.py > definitions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb0fa7",
   "metadata": {},
   "source": [
    "Now we have the `definitions.py` file, where the categories are defined as a Python dictionary, and the dictionary can be conveniently imported into this Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38825e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acc-phys',\n",
       "  {'name': 'Accelerator Physics',\n",
       "   'in_archive': 'acc-phys',\n",
       "   'is_active': False,\n",
       "   'is_general': False}),\n",
       " ('adap-org',\n",
       "  {'name': 'Adaptation, Noise, and Self-Organizing Systems',\n",
       "   'in_archive': 'adap-org',\n",
       "   'is_active': False,\n",
       "   'is_general': False}),\n",
       " ('alg-geom',\n",
       "  {'name': 'Algebraic Geometry',\n",
       "   'in_archive': 'alg-geom',\n",
       "   'is_active': False,\n",
       "   'is_general': False})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from definitions import CATEGORIES\n",
    "\n",
    "\n",
    "# Check out three of the categories:\n",
    "list(CATEGORIES.items())[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67fffe",
   "metadata": {},
   "source": [
    "The categories are identified with codes like `acc-phys`, `adap-org` and `alg-geom`, and they have also more human-readable names. Next we use the categories to construct [subject vocabulary](https://github.com/NatLibFi/Annif/wiki/Subject-vocabulary-formats) in TSV format.\n",
    "\n",
    "In Annif subject vocabulary the subjects are identified by URIs. As the arXiv taxonomy does not use URIs, we create (dummy) URIs based on the category codes by prepending each category code with `https://arxiv.org/list/`, which makes the URIs act as URLs to pages on arXiv.org website. For Annif vocabulary the URIs are surrounded with angle brackets (`<>`). For the labels of the subjects we use the category names. \n",
    "\n",
    "There are [some categories](https://github.com/arXiv/arxiv-base/blob/163c1f9ddb60a017b21ff03190662ea171884530/arxiv/taxonomy/definitions.py#L2100-L2147) apparently used for testing purposes and some have been deprecated (they have field `'is_active': False`), so we omit them from our vocabulary.\n",
    "\n",
    "The vocabulary is saved to the file `arxiv-vocab.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8bd2586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<https://arxiv.org/list/astro-ph.CO>\tCosmology and Nongalactic Astrophysics\n",
      "<https://arxiv.org/list/astro-ph.EP>\tEarth and Planetary Astrophysics\n",
      "<https://arxiv.org/list/astro-ph.GA>\tAstrophysics of Galaxies\n",
      "<https://arxiv.org/list/astro-ph.HE>\tHigh Energy Astrophysical Phenomena\n",
      "<https://arxiv.org/list/astro-ph.IM>\tInstrumentation and Methods for Astrophysics\n",
      "<https://arxiv.org/list/astro-ph.SR>\tSolar and Stellar Astrophysics\n",
      "<https://arxiv.org/list/cond-mat.dis-nn>\tDisordered Systems and Neural Networks\n",
      "<https://arxiv.org/list/cond-mat.mes-hall>\tMesoscale and Nanoscale Physics\n",
      "<https://arxiv.org/list/cond-mat.mtrl-sci>\tMaterials Science\n",
      "<https://arxiv.org/list/cond-mat.other>\tOther Condensed Matter\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "URI_BASE = 'https://arxiv.org/list/'\n",
    "\n",
    "with open('arxiv-vocab.tsv', 'w', encoding='utf-8') as vocab_file:\n",
    "    for category, data in CATEGORIES.items():\n",
    "        if data['in_archive'] == 'test':\n",
    "            continue  # Skip categories belonging to the 'test' archive\n",
    "        if not data['is_active']:\n",
    "            continue  # Skip deprecated categories\n",
    "        print('<' + URI_BASE + category + '>\\t' + data['name'], file=vocab_file) \n",
    "\n",
    "\n",
    "# We can check the first 10 lines of the file with `head` command:\n",
    "! head arxiv-vocab.tsv\n",
    "# And count lines in the file with `wc -l` command:\n",
    "! wc -l < arxiv-vocab.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516f317",
   "metadata": {},
   "source": [
    "We have 155 subjects in our arXiv vocabulary.\n",
    "\n",
    "\n",
    "## Document corpus\n",
    "\n",
    "The actual document corpus still needs to be constructed. Along this tutorial is provided a sample (100k articles) from an arXiv dataset that is distributed in [Kaggle](https://www.kaggle.com/Cornell-University/arxiv) (you can freely download the full data-set of 1.7M+ articles if you wish). \n",
    "\n",
    "Our data-set is compressed with `gzip`, so the first step is to unpack it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66cff8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gzip -d arxiv-metadata-oai-snapshot-100k-articles.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b39c73",
   "metadata": {},
   "source": [
    "Now we have the metadata in the file `arxiv-metadata-oai-snapshot-100k-articles.json`. Take a look at the file by printing the first two lines of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e63bad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"0704.0001\",\"submitter\":\"Pavel Nadolsky\",\"authors\":\"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\"title\":\"Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies\",\"comments\":\"37 pages, 15 figures; published version\",\"journal-ref\":\"Phys.Rev.D76:013009,2007\",\"doi\":\"10.1103/PhysRevD.76.013009\",\"report-no\":\"ANL-HEP-PR-07-12\",\"categories\":\"hep-ph\",\"license\":null,\"abstract\":\"  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Mon, 2 Apr 2007 19:18:42 GMT\"},{\"version\":\"v2\",\"created\":\"Tue, 24 Jul 2007 20:10:27 GMT\"}],\"update_date\":\"2008-11-26\",\"authors_parsed\":[[\"Bal\\u00e1zs\",\"C.\",\"\"],[\"Berger\",\"E. L.\",\"\"],[\"Nadolsky\",\"P. M.\",\"\"],[\"Yuan\",\"C. -P.\",\"\"]]}\r\n",
      "{\"id\":\"0704.0002\",\"submitter\":\"Louis Theran\",\"authors\":\"Ileana Streinu and Louis Theran\",\"title\":\"Sparsity-certifying Graph Decompositions\",\"comments\":\"To appear in Graphs and Combinatorics\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"math.CO cs.CG\",\"license\":\"http://arxiv.org/licenses/nonexclusive-distrib/1.0/\",\"abstract\":\"  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 02:26:18 GMT\"},{\"version\":\"v2\",\"created\":\"Sat, 13 Dec 2008 17:26:00 GMT\"}],\"update_date\":\"2008-12-13\",\"authors_parsed\":[[\"Streinu\",\"Ileana\",\"\"],[\"Theran\",\"Louis\",\"\"]]}\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 arxiv-metadata-oai-snapshot-100k-articles.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b2f17",
   "metadata": {},
   "source": [
    "The file is in [newline-delimited JSON](http://ndjson.org/) format: each line has one JSON object representing the metadata of an article. Each article will become a document in our corpus.\n",
    "\n",
    "The JSON object for the article metadata has several fields (`id`, `submitter`, `author` etc.), but we are interested in three of them:\n",
    "- `categories` include the category codes which will be mapped to our vocabulary subjects and used as the (gold-standard) subjects of a document,\n",
    "- `title` and `abstract` are used to form the text content of a document.\n",
    "\n",
    "First we construct a Python dictionary, which will be used to map category code to URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ff66971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<https://arxiv.org/list/bayes-an>'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat2uri = {}\n",
    "\n",
    "for category in CATEGORIES.keys():\n",
    "    cat2uri[category] = '<' + URI_BASE + category + '>'\n",
    "\n",
    "# For example 'bayes-an' catecory code is mapped to '<https://arxiv.org/list/bayes-an>' URI:\n",
    "cat2uri['bayes-an']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9684cb",
   "metadata": {},
   "source": [
    "The JSON objects can be parsed with Python using `loads()` funtion of the `json` library, so we import the library.\n",
    "\n",
    "As there are newline characters (`\\n`) in the titles and abstracts we also define a simple function for replacing them and all other whitespace characters with spaces (tabulator characters would cause problems in the corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04d1f212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...chromodynamics is presented for...'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# For example:\n",
    "cleanup('...chromodynamics is\\npresented for...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f4ac6",
   "metadata": {},
   "source": [
    "Now we are ready to create the corpus!\n",
    "\n",
    "\n",
    "We use [Short text document corpus format](https://github.com/NatLibFi/Annif/wiki/Document-corpus-formats#short-text-document-corpus-tsv-file), which is the same as the subject vocabulary format: \n",
    "a TSV file, where the first column contains the text of the document, and the second column contains a whitespace-separated list of subject URIs for the document.\n",
    "\n",
    "The text of the documents is formed by concatenating the title and abstract. If an article would have an category that is not in our vocabulary, a warning is printed out (`Unknown category: XXX`) and that article is omitted from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a783a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file for reading the input JSON metadata file:\n",
    "with open('arxiv-metadata-oai-snapshot-100k-articles.json', 'r', encoding='utf-8') as input_metadata_file:\n",
    "    # Open file for writing the output TSV corpus file:\n",
    "    with open('arxiv-corpus.tsv', 'w', encoding='utf-8') as output_corpus_file:\n",
    "        # Loop line-by-line:\n",
    "        for line in input_metadata_file:\n",
    "            article_metadata = json.loads(line)\n",
    "            text = cleanup(\n",
    "                article_metadata['title'] + article_metadata['abstract'])\n",
    "            try:\n",
    "                uris = [cat2uri[cat] for cat in article_metadata['categories'].split()]\n",
    "            except KeyError:\n",
    "                print('Unknown category: ' + article_metadata['categories'])\n",
    "                continue\n",
    "            print(text + '\\t' + ' '.join(uris), file=output_corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cffb64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of prompt diphoton production cross sections at Tevatron and LHC energies A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events.\t<https://arxiv.org/list/hep-ph>\r\n",
      "Sparsity-certifying Graph Decompositions We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. Special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. In particular, our colored pebbles generalize and strengthen the previous results of Lee and Streinu and give a new proof of the Tutte-Nash-Williams characterization of arboricity. We also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. Our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and Westermann and Hendrickson.\t<https://arxiv.org/list/math.CO> <https://arxiv.org/list/cs.CG>\r\n",
      "The evolution of the Earth-Moon system based on the dark matter field fluid model The evolution of Earth-Moon system is described by the dark matter field fluid model proposed in the Meeting of Division of Particle and Field 2004, American Physical Society. The current behavior of the Earth-Moon system agrees with this model very well and the general pattern of the evolution of the Moon-Earth system described by this model agrees with geological and fossil evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5 billion years ago, which is far beyond the Roche's limit. The result suggests that the tidal friction may not be the primary cause for the evolution of the Earth-Moon system. The average dark matter field fluid constant derived from Earth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts that the Mars's rotation is also slowing with the angular acceleration rate about -4.38 x 10^(-22) rad s^(-2).\t<https://arxiv.org/list/physics.gen-ph>\r\n"
     ]
    }
   ],
   "source": [
    "# Check out how few of the documents in the corpus looks like:\n",
    "! head -n 3 arxiv-corpus.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09cfb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\r\n"
     ]
    }
   ],
   "source": [
    "# And how many documents there are in the corpus:\n",
    "! wc -l < arxiv-corpus.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e402e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
